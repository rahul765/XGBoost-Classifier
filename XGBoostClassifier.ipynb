{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_classification = pd.read_csv(\"Social_Network_Ads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>15624510</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15810944</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>15668575</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>15603246</td>\n",
       "      <td>Female</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>15804002</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User ID  Gender  Age  EstimatedSalary  Purchased\n",
       "0  15624510    Male   19            19000          0\n",
       "1  15810944    Male   35            20000          0\n",
       "2  15668575  Female   26            43000          0\n",
       "3  15603246  Female   27            57000          0\n",
       "4  15804002    Male   19            76000          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_classification.drop(['User ID'], inplace = True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_classification.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_classification['Gender'] = le.fit_transform(dataset_classification['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age  EstimatedSalary  Purchased\n",
       "0       1   19            19000          0\n",
       "1       1   35            20000          0\n",
       "2       0   26            43000          0\n",
       "3       0   27            57000          0\n",
       "4       1   19            76000          0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset_classification['Purchased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_classification.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost ( eXtreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is XGBoost good ?\n",
    "\n",
    "    1. Parallel Computing: It is enabled with parallel processing (using OpenMP); i.e., when you run xgboost, by default, it would use all the cores of your laptop/machine.\n",
    "    \n",
    "    2. Regularization: I believe this is the biggest advantage of xgboost. GBM has no provision for regularization. Regularization is a technique used to avoid overfitting in linear and tree-based models.\n",
    "    \n",
    "    3. Enabled Cross Validation: In R, we usually use external packages such as caret and mlr to obtain CV results. But, xgboost is enabled with internal CV function.\n",
    "    \n",
    "    4. Missing Values: XGBoost is designed to handle missing values internally. The missing values are treated in such a manner that if there exists any trend in missing values, it is captured by the model.\n",
    "    \n",
    "    5. Flexibility: In addition to regression, classification, and ranking problems, it supports user-defined objective functions also. An objective function is used to measure the performance of the model given a certain set of parameters. Furthermore, it supports user defined evaluation metrics as well.\n",
    "    \n",
    "    6. Availability: Currently, available for programming languages such as R, Python, Java, Julia, and Scala.\n",
    "    \n",
    "    7. Save and Reload: XGBoost gives us a feature to save our data matrix and model and reload it later. Suppose, we have a large data set, we can simply save the model and use it in future instead of wasting time redoing the computation.\n",
    "    \n",
    "    8. Tree Pruning: Unlike GBM, where tree pruning stops once a negative loss is encountered, XGBoost grows the tree upto max_depth and then prune backward until the improvement in loss function is below a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type of problems XGBoost can solve :\n",
    "\n",
    "##### 1. Classification : \n",
    "    a. It uses booster = gbtree parameter; i.e., a tree is grown one after other and attempts to reduce misclassification rate in subsequent iterations. \n",
    "    b. In this, the next tree is built by giving a higher weight to misclassified points by the previous tree.\n",
    "    \n",
    "##### 2. Regression :\n",
    "    a. we have two methods: booster = gbtree and booster = gblinear. \n",
    "    b. In gblinear, it builds generalized linear model and optimizes it using regularization (L1,L2) and gradient descent. \n",
    "    c. In this, the subsequent models are built on residuals (actual - predicted) generated by previous iterations.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding XGBoost Tuning Parameters:\n",
    "\n",
    "XGBoost parameters can be divided into three categories (as suggested by its authors):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    1. General Parameters: Controls the booster type in the model which eventually drives overall functioning are as follows:\n",
    "        a. booster[default=gbtree]\n",
    "                Sets the booster type (gbtree, gblinear or dart) to use. \n",
    "                    For classification problems, you can use gbtree, dart. \n",
    "                    For regression, you can use any.\n",
    "        b. n_jobs[default=1]\n",
    "                Activates parallel computation. \n",
    "                Generally, people don't change it as using maximum cores leads to the fastest computation. \n",
    "        c. silent[default=None]\n",
    "            If you set it to 1, your console will get flooded with running messages.\n",
    "            Better not to change it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Booster Parameters\n",
    "As mentioned above, parameters for tree and linear boosters are different. Let's understand each one of them:\n",
    "\n",
    "######    A. Parameters for Tree Booster\n",
    "        a. n_estimators[default=100]\n",
    "            It controls the maximum number of iterations. \n",
    "            For classification, it is similar to the number of trees to grow.\n",
    "            Should be tuned using CV\n",
    "        b. learning_rate[default=0.1][range: (0,1)]\n",
    "            It controls the learning rate, i.e., the rate at which our model learns patterns in data. \n",
    "            After every round, it shrinks the feature weights to reach the best optimum.\n",
    "            Lower eta leads to slower computation. It must be supported by increase in nrounds.\n",
    "            Typically, it lies between 0.01 - 0.3\n",
    "        c. gamma[default=0][range: (0,Inf)]\n",
    "            It controls regularization (or prevents overfitting). \n",
    "            The optimal value of gamma depends on the data set and other parameter values.\n",
    "            Higher the value, higher the regularization. \n",
    "            Regularization means penalizing large coefficients which don't improve the model's performance. \n",
    "            Default = 0 means no regularization.\n",
    "            Tune trick: Start with 0 and check CV error rate. \n",
    "                If you see train error >>> test error, bring gamma into action. \n",
    "                Higher the gamma, lower the difference in train and test CV. \n",
    "                If you have no clue what value to use, use gamma=5 and see the performance. \n",
    "                Remember that gamma brings improvement when you want to use shallow (low max_depth) trees.\n",
    "        d. max_depth[default=3][range: (0,Inf)] \n",
    "            It controls the depth of the tree.\n",
    "            Larger the depth, more complex the model; higher chances of overfitting. \n",
    "            There is no standard value for max_depth. Larger data sets require deep trees to learn the rules from data.\n",
    "            Should be tuned using CV\n",
    "        e. min_child_weight[default=1][range:(0,Inf)]\n",
    "            In regression, it refers to the minimum number of instances required in a child node. \n",
    "            In classification, if the leaf node has a minimum sum of instance weight (calculated by second order partial derivative) lower than min_child_weight, the tree splitting stops.\n",
    "            In simple words, it blocks the potential feature interactions to prevent overfitting. Should be tuned using CV.\n",
    "        f. subsample[default=1][range: (0,1)]\n",
    "            It controls the number of samples (observations) supplied to a tree.\n",
    "            Typically, its values lie between (0.5-0.8)\n",
    "        g. colsample_bytree[default=1][range: (0,1)]\n",
    "            It control the number of features (variables) supplied to a tree\n",
    "            Typically, its values lie between (0.5,0.9)\n",
    "        h. reg_lambda[default=1]\n",
    "            It controls L2 regularization (equivalent to Ridge regression) on weights. \n",
    "            It is used to avoid overfitting.\n",
    "        i. reg_alpha[default=0]\n",
    "            It controls L1 regularization (equivalent to Lasso regression) on weights. \n",
    "            In addition to shrinkage, enabling alpha also results in feature selection. \n",
    "            Hence, it's more useful on high dimensional data sets.\n",
    "\n",
    "\n",
    "##### 2. Parameters for Linear Booster\n",
    "Using linear booster has relatively lesser parameters to tune, hence it computes much faster than gbtree booster.\n",
    "\n",
    "    a. n_estimators[default=100]\n",
    "        It controls the maximum number of iterations (steps) required for gradient descent to converge.\n",
    "        Should be tuned using CV\n",
    "    b. reg_lambda[default=1]\n",
    "        It enables Ridge Regression. Same as above\n",
    "    c. reg_alpha[default=0]\n",
    "        It enables Lasso Regression. Same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Learning Task Parameters: \n",
    "Sets and evaluates the learning process of the booster from the given data\n",
    "\n",
    "These parameters specify methods for the loss function and model evaluation. In addition to the parameters listed below, you are free to use a customized objective / evaluation function.\n",
    "\n",
    "    a. objective[default=binary:logistic]\n",
    "            reg:linear - for linear regression\n",
    "            binary:logistic - logistic regression for binary classification. It returns class probabilities\n",
    "            multi:softmax - multiclassification using softmax objective. \n",
    "                            It returns predicted class labels. \n",
    "                            It requires setting num_class parameter denoting number of unique prediction classes.\n",
    "            multi:softprob - multiclassification using softmax objective. It returns predicted class probabilities.\n",
    "    b. eval_metric [no default, depends on objective selected] - not in python\n",
    "            These metrics are used to evaluate a model's accuracy on validation data. \n",
    "                For regression, default metric is RMSE. \n",
    "                For classification, default metric is error.\n",
    "                    Available error functions are as follows:\n",
    "                        mae - Mean Absolute Error (used in regression)\n",
    "                        Logloss - Negative loglikelihood (used in classification)\n",
    "                        AUC - Area under curve (used in classification)\n",
    "                        RMSE - Root mean square error (used in regression)\n",
    "                        error - Binary classification error rate [#wrong cases/#all cases]\n",
    "                        mlogloss - multiclass logloss (used in classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=10,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9242636746143058"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrf = XGBRFClassifier(random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRFClassifier(base_score=0.5, colsample_bylevel=1, colsample_bynode=0.8,\n",
       "                colsample_bytree=1, gamma=0, learning_rate=1, max_delta_step=0,\n",
       "                max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "                n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "                random_state=10, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                seed=None, silent=None, subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbrf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgbrf = xgbrf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9242636746143058"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_xgbrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_xgbrf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
